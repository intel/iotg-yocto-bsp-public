From e431e3a65e7af09a06b5fd0b2ad04e7c692015ad Mon Sep 17 00:00:00 2001
From: Jayasankar <jayasankarx.nara@intel.com>
Date: Wed, 16 Nov 2016 09:38:05 +0800
Subject: [PATCH 32/42] igb: Scheduling suspend in transmit side based on
 activity monitoring

Invoking resume if transmit handler is called in suspended
state and scheduling suspend if there is no tx pending packets.

Signed-off-by: Jayasankar <jayasankarx.nara@intel.com>
Signed-off-by: Lai, Poey Seng <poey.seng.lai@intel.com>
Reviewed-by: Vinod Kumar <vinod.kumarx.vinod.kumar@intel.com>
---
 drivers/net/ethernet/intel/igb/igb.h      |  5 +++
 drivers/net/ethernet/intel/igb/igb_main.c | 72 ++++++++++++++++++++++++++++---
 2 files changed, 72 insertions(+), 5 deletions(-)

diff --git a/drivers/net/ethernet/intel/igb/igb.h b/drivers/net/ethernet/intel/igb/igb.h
index 4ad6ac1..88de3f1 100644
--- a/drivers/net/ethernet/intel/igb/igb.h
+++ b/drivers/net/ethernet/intel/igb/igb.h
@@ -523,11 +523,13 @@ struct igb_adapter {
 	u32 en_mng_pt;
 	u16 link_speed;
 	u16 link_duplex;
+	u32 igb_tx_pending;
 
 	u8 __iomem *io_addr; /* Mainly for iounmap use */
 
 	struct work_struct reset_task;
 	struct work_struct watchdog_task;
+	struct work_struct rpm_xmit_task;
 	bool fc_autoneg;
 	u8  tx_timeout_factor;
 	struct timer_list blink_timer;
@@ -537,6 +539,9 @@ struct igb_adapter {
 	struct pci_dev *pdev;
 
 	spinlock_t stats64_lock;
+	/* spin lock for tx pending count */
+	spinlock_t rpm_txlock;
+
 	struct rtnl_link_stats64 stats64;
 
 	/* structs defined in e1000_hw.h */
diff --git a/drivers/net/ethernet/intel/igb/igb_main.c b/drivers/net/ethernet/intel/igb/igb_main.c
index ffc45ca..588f040 100644
--- a/drivers/net/ethernet/intel/igb/igb_main.c
+++ b/drivers/net/ethernet/intel/igb/igb_main.c
@@ -148,6 +148,7 @@ enum tx_queue_prio {
 static void igb_update_phy_info(unsigned long);
 static void igb_watchdog(unsigned long);
 static void igb_watchdog_task(struct work_struct *);
+static void igb_rpm_xmit_task(struct work_struct *);
 static netdev_tx_t igb_xmit_frame(struct sk_buff *skb, struct net_device *);
 static void igb_get_stats64(struct net_device *dev,
 			    struct rtnl_link_stats64 *stats);
@@ -3046,7 +3047,7 @@ static int igb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	INIT_WORK(&adapter->reset_task, igb_reset_task);
 	INIT_WORK(&adapter->watchdog_task, igb_watchdog_task);
-
+	INIT_WORK(&adapter->rpm_xmit_task, igb_rpm_xmit_task);
 	/* Initialize link properties that are user-changeable */
 	adapter->fc_autoneg = true;
 	hw->mac.autoneg = true;
@@ -3632,7 +3633,8 @@ static int igb_sw_init(struct igb_adapter *adapter)
 				  VLAN_HLEN;
 	adapter->min_frame_size = ETH_ZLEN + ETH_FCS_LEN;
 
-	spin_lock_init(&adapter->nfc_lock);
+	adapter->igb_tx_pending = 0;
+	spin_lock_init(&adapter->rpm_txlock);
 	spin_lock_init(&adapter->stats64_lock);
 #ifdef CONFIG_PCI_IOV
 	switch (hw->mac.type) {
@@ -5498,10 +5500,12 @@ static void igb_tx_ctxtdesc(struct igb_ring *tx_ring,
 	struct e1000_adv_tx_context_desc *context_desc;
 	u16 i = tx_ring->next_to_use;
 	struct timespec64 ts;
+	struct igb_adapter *adapter = netdev_priv(tx_ring->netdev);
 
 	context_desc = IGB_TX_CTXTDESC(tx_ring, i);
 
 	i++;
+	adapter->igb_tx_pending++;
 	tx_ring->next_to_use = (i < tx_ring->count) ? i : 0;
 
 	/* set bits to identify this as an advanced context descriptor */
@@ -5761,12 +5765,13 @@ static int igb_tx_map(struct igb_ring *tx_ring,
 	struct igb_tx_buffer *tx_buffer;
 	union e1000_adv_tx_desc *tx_desc;
 	struct skb_frag_struct *frag;
+	struct igb_adapter *adapter = netdev_priv(tx_ring->netdev);
 	dma_addr_t dma;
 	unsigned int data_len, size;
 	u32 tx_flags = first->tx_flags;
 	u32 cmd_type = igb_tx_cmd_type(skb, tx_flags);
 	u16 i = tx_ring->next_to_use;
-
+	u32 count = 0;
 	tx_desc = IGB_TX_DESC(tx_ring, i);
 
 	igb_tx_olinfo_status(tx_ring, tx_desc, tx_flags, skb->len - hdr_len);
@@ -5794,6 +5799,7 @@ static int igb_tx_map(struct igb_ring *tx_ring,
 
 			i++;
 			tx_desc++;
+			count++;
 			if (i == tx_ring->count) {
 				tx_desc = IGB_TX_DESC(tx_ring, 0);
 				i = 0;
@@ -5813,6 +5819,7 @@ static int igb_tx_map(struct igb_ring *tx_ring,
 
 		i++;
 		tx_desc++;
+		count++;
 		if (i == tx_ring->count) {
 			tx_desc = IGB_TX_DESC(tx_ring, 0);
 			i = 0;
@@ -5855,6 +5862,7 @@ static int igb_tx_map(struct igb_ring *tx_ring,
 
 	tx_ring->next_to_use = i;
 
+	adapter->igb_tx_pending += count;
 	/* Make sure there is space in the ring for the next send. */
 	igb_maybe_stop_tx(tx_ring, DESC_NEEDED);
 
@@ -6000,10 +6008,32 @@ static inline struct igb_ring *igb_tx_queue_mapping(struct igb_adapter *adapter,
 	return adapter->tx_ring[r_idx];
 }
 
+/* We cannot invoke resume from igb_xmit_frame as this function is getting
+ * called under a spinlock, and pm_runtime_get_sync flow is calling msleep
+ * in pci driver code, hence causing "BUG: scheduling while atomic:"
+ * so using this work task to invoke resume
+ */
+static void igb_rpm_xmit_task(struct work_struct *work)
+{
+	struct igb_adapter *adapter = container_of(work,
+						   struct igb_adapter,
+						   rpm_xmit_task);
+	struct net_device *netdev = adapter->netdev;
+	struct pci_dev *pdev = adapter->pdev;
+
+	pm_runtime_get_sync(&pdev->dev);
+
+	if (netif_queue_stopped(netdev))
+		netif_tx_wake_all_queues(netdev);
+	pm_runtime_put_noidle(&pdev->dev);
+}
+
 static netdev_tx_t igb_xmit_frame(struct sk_buff *skb,
 				  struct net_device *netdev)
 {
 	struct igb_adapter *adapter = netdev_priv(netdev);
+	struct pci_dev *pdev = adapter->pdev;
+	netdev_tx_t netdev_status = NETDEV_TX_OK;
 
 	/* The minimum packet size with TCTL.PSP set is 17 so pad the skb
 	 * in order to meet this minimum size requirement.
@@ -6011,7 +6041,19 @@ static netdev_tx_t igb_xmit_frame(struct sk_buff *skb,
 	if (skb_put_padto(skb, 17))
 		return NETDEV_TX_OK;
 
-	return igb_xmit_frame_ring(skb, igb_tx_queue_mapping(adapter, skb));
+	if (!adapter->igb_tx_pending) {
+		if (!pm_runtime_active(&pdev->dev)) {
+			netif_tx_stop_all_queues(netdev);
+				schedule_work(&adapter->rpm_xmit_task);
+			return NETDEV_TX_BUSY;
+		}
+		pm_runtime_get_noresume(&pdev->dev);
+	}
+	spin_lock(&adapter->rpm_txlock);
+	netdev_status = igb_xmit_frame_ring(skb,
+					    igb_tx_queue_mapping(adapter, skb));
+	spin_unlock(&adapter->rpm_txlock);
+	return netdev_status;
 }
 
 /**
@@ -7602,11 +7644,15 @@ static bool igb_clean_tx_irq(struct igb_q_vector *q_vector, int napi_budget)
 	unsigned int total_bytes = 0, total_packets = 0;
 	unsigned int budget = q_vector->tx.work_limit;
 	unsigned int i = tx_ring->next_to_clean;
+	union e1000_adv_tx_desc *eop_desc_temp = NULL;
+	unsigned int count = 0;
+	struct pci_dev *pdev = adapter->pdev;
 
 	if (test_bit(__IGB_DOWN, &adapter->state))
 		return true;
 
 	tx_buffer = &tx_ring->tx_buffer_info[i];
+	eop_desc_temp = tx_buffer->next_to_watch;
 	tx_desc = IGB_TX_DESC(tx_ring, i);
 	i -= tx_ring->count;
 
@@ -7648,6 +7694,7 @@ static bool igb_clean_tx_irq(struct igb_q_vector *q_vector, int napi_budget)
 			tx_buffer++;
 			tx_desc++;
 			i++;
+			count++;
 			if (unlikely(!i)) {
 				i -= tx_ring->count;
 				tx_buffer = tx_ring->tx_buffer_info;
@@ -7753,7 +7800,18 @@ static bool igb_clean_tx_irq(struct igb_q_vector *q_vector, int napi_budget)
 			u64_stats_update_end(&tx_ring->tx_syncp);
 		}
 	}
-
+/* schedule suspend if tx pending count is zero */
+	if (eop_desc_temp) {
+		spin_lock(&adapter->rpm_txlock);
+		adapter->igb_tx_pending -= count;
+		if (!adapter->igb_tx_pending) {
+			spin_unlock(&adapter->rpm_txlock);
+			pm_runtime_mark_last_busy(&pdev->dev);
+			pm_runtime_put_sync_autosuspend(&pdev->dev);
+		} else {
+			spin_unlock(&adapter->rpm_txlock);
+		}
+	}
 	return !!budget;
 }
 
@@ -8787,7 +8845,11 @@ static int __maybe_unused igb_runtime_suspend(struct device *dev)
 	struct pci_dev *pdev = to_pci_dev(dev);
 	int retval;
 	bool wake;
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct igb_adapter *adapter = netdev_priv(netdev);
 
+	if (adapter->igb_tx_pending)
+		return -EBUSY;
 	retval = __igb_shutdown(pdev, &wake, 1);
 	if (retval)
 		return retval;
-- 
1.9.1

